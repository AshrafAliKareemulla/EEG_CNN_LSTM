{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9cce4109",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "4cd6f0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Conv Block 1\n",
    "        self.conv1 = nn.Conv2d(5, 64, kernel_size=(5,5), padding=(2,2), stride=(1,1))\n",
    "        self.conv2 = nn.Conv2d(64, 64, kernel_size=(5,5), padding=(2,2), stride=(1,1))\n",
    "        self.conv3 = nn.Conv2d(64, 64, kernel_size=(5,5), padding=(2,2), stride=(1,1))\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "        self.dropout1 = nn.Dropout2d(0.3)\n",
    "        \n",
    "        # Conv Block 2\n",
    "        self.conv4 = nn.Conv2d(64, 128, kernel_size=(5,5), padding=(2,2), stride=(1,1))\n",
    "        self.conv5 = nn.Conv2d(128, 128, kernel_size=(5,5), padding=(2,2), stride=(1,1))\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "        self.dropout2 = nn.Dropout2d(0.2)\n",
    "        \n",
    "        # Conv Block 3\n",
    "        self.conv6 = nn.Conv2d(128, 256, kernel_size=(3,3), padding=(1,1), stride=(1,1))\n",
    "        self.maxpool3 = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "        self.dropout3 = nn.Dropout2d(0.25)\n",
    "        \n",
    "        # Conv Block 4\n",
    "        self.conv7 = nn.Conv2d(256, 512, kernel_size=(3,3), padding=(1,1), stride=(1,1))\n",
    "        self.maxpool4 = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "        self.dropout4 = nn.Dropout2d(0.3)\n",
    "        \n",
    "        # FFNN\n",
    "        self.fc1 = nn.Linear(3*16*512, 512)\n",
    "        self.dropout5 = nn.Dropout(0.4)\n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.dropout6 = nn.Dropout(0.2)\n",
    "        self.fc3 = nn.Linear(256, 64)\n",
    "        self.dropout7 = nn.Dropout(0.1)\n",
    "        self.fc4 = nn.Linear(64, 3)\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.dropout1(self.maxpool1(x))\n",
    "        \n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = self.dropout2(self.maxpool2(x))\n",
    "        \n",
    "        x = F.relu(self.conv6(x))\n",
    "        x = self.dropout3(self.maxpool3(x))\n",
    "        \n",
    "        x = F.relu(self.conv7(x))\n",
    "        x = self.dropout4(self.maxpool4(x))\n",
    "        \n",
    "        x = x.view(-1, 3*16*512)\n",
    "        \n",
    "        x = self.dropout5(F.relu(self.fc1(x)))\n",
    "        x = self.dropout6(F.relu(self.fc2(x)))\n",
    "        x = self.dropout7(F.relu(self.fc3(x)))\n",
    "        \n",
    "        x = self.fc4(x)\n",
    "        \n",
    "        return torch.softmax(x, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "2c75a906",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 3])\n",
      "15034371\n"
     ]
    }
   ],
   "source": [
    "obj = CNN()\n",
    "print(obj(torch.randn(16, 5, 62, 265)).shape)\n",
    "print(sum(n.numel() for n in obj.parameters() if n.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "0ecf24fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.lstm1 = nn.LSTM(62, 64, num_layers = 1, batch_first = True, dropout = 0.25)\n",
    "        self.lstm2 = nn.LSTM(64, 128, num_layers = 1, batch_first = True, dropout = 0.3)\n",
    "        self.lstm3 = nn.LSTM(128, 128, num_layers = 1, batch_first = True, dropout = 0.35)\n",
    "        self.lstm4 = nn.LSTM(128, 256, num_layers = 1, batch_first = True, dropout = 0.4)\n",
    "        \n",
    "        self.fc1 = nn.Linear(256*1325, 512)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.dropout2 = nn.Dropout(0.35)\n",
    "        \n",
    "        self.fc3 = nn.Linear(256, 64)\n",
    "        self.fc4 = nn.Linear(64, 3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x, _ = self.lstm4(x)\n",
    "        \n",
    "        x = x.contiguous().view(-1, 256 * 1325)\n",
    "        \n",
    "        x = self.dropout1(F.relu(self.fc1(x)))\n",
    "        x = self.dropout2(F.relu(self.fc2(x)))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return torch.softmax(x, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "a8db8b92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 3])\n",
      "174478339\n"
     ]
    }
   ],
   "source": [
    "obj1 = LSTM()\n",
    "print(obj1(torch.randn(4, 1325, 62)).shape)\n",
    "print(sum(n.numel() for n in obj1.parameters() if n.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7c864b16",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Hybrid_CNN_LSTM(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        # Conv Block 1\n",
    "        self.conv1 = nn.Conv2d(5, 64, kernel_size=(5,5), padding=(2,2), stride=(1,1))\n",
    "        self.conv2 = nn.Conv2d(64, 128, kernel_size=(5,5), padding=(2,2), stride=(1,1))\n",
    "        self.conv3 = nn.Conv2d(128, 128, kernel_size=(5,5), padding=(2,2), stride=(1,1))\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "        self.dropout1 = nn.Dropout(0.3)\n",
    "        self.bn1 = nn.BatchNorm2d(128)\n",
    "        \n",
    "        \n",
    "        # Conv Block 2\n",
    "        self.conv4 = nn.Conv2d(128, 256, kernel_size=(5,5), padding=(2,2), stride=(1,1))\n",
    "        self.conv5 = nn.Conv2d(256, 256, kernel_size=(5,5), padding=(2,2), stride=(1,1))\n",
    "        self.maxpool2 = nn.MaxPool2d(kernel_size=(2,2), stride=(2,2))\n",
    "        self.dropout2 = nn.Dropout(0.25)\n",
    "        self.bn2 = nn.BatchNorm2d(256)\n",
    "        \n",
    "        # Conv Block 3\n",
    "        self.conv6 = nn.Conv2d(256, 512, kernel_size=(3,3), padding=(1,1), stride=(1,1))\n",
    "        self.bn3 = nn.BatchNorm2d(512)\n",
    "        \n",
    "\n",
    "        # LSTM blocks\n",
    "        self.lstm1 = nn.LSTM(66, 64, num_layers = 1, batch_first = True, dropout = 0.25)\n",
    "        self.lstm2 = nn.LSTM(64, 128, num_layers = 1, batch_first = True, dropout = 0.3)\n",
    "        self.lstm3 = nn.LSTM(128, 256, num_layers = 1, batch_first = True, dropout = 0.35)\n",
    "        self.lstm4 = nn.LSTM(256, 512, num_layers = 1, batch_first = True, dropout = 0.4)\n",
    "\n",
    "        # FFNNN\n",
    "        self.fc1 = nn.Linear(66*512, 512)\n",
    "        self.dropout1 = nn.Dropout(0.25)\n",
    "        \n",
    "        self.fc2 = nn.Linear(512, 256)\n",
    "        self.dropout2 = nn.Dropout(0.3)\n",
    "        \n",
    "        self.fc3 = nn.Linear(256, 64)\n",
    "        self.fc4 = nn.Linear(64, 3)\n",
    "        \n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = self.bn1(self.dropout1(self.maxpool1(x)))\n",
    "        \n",
    "        x = F.relu(self.conv4(x))\n",
    "        x = F.relu(self.conv5(x))\n",
    "        x = self.bn2(self.dropout2(self.maxpool2(x)))\n",
    "        \n",
    "        x = self.bn3(F.relu(self.conv6(x)))\n",
    "\n",
    "        x = x.permute(0, 3, 1, 2)\n",
    "\n",
    "        x = x.reshape(x.shape[0], x.shape[1], -1)\n",
    "\n",
    "        print(x.shape)\n",
    "        x, _ = self.lstm1(x)\n",
    "        x, _ = self.lstm2(x)\n",
    "        x, _ = self.lstm3(x)\n",
    "        x, _ = self.lstm4(x)\n",
    "\n",
    "        print(x.shape)\n",
    "        \n",
    "        x = x.contiguous().view(-1, 66 * 512)\n",
    "        \n",
    "        x = self.dropout1(F.relu(self.fc1(x)))\n",
    "        x = self.dropout2(F.relu(self.fc2(x)))\n",
    "        x = F.relu(self.fc3(x))\n",
    "        x = self.fc4(x)\n",
    "\n",
    "        return torch.softmax(x, dim = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8b92cddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 66, 7680])\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "input.size(-1) must be equal to input_size. Expected 66, got 7680",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m obj2 \u001b[38;5;241m=\u001b[39m Hybrid_CNN_LSTM()\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mobj2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m62\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m265\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mshape)\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28msum\u001b[39m(n\u001b[38;5;241m.\u001b[39mnumel() \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m obj2\u001b[38;5;241m.\u001b[39mparameters() \u001b[38;5;28;01mif\u001b[39;00m n\u001b[38;5;241m.\u001b[39mrequires_grad))\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[1;32mIn[16], line 61\u001b[0m, in \u001b[0;36mHybrid_CNN_LSTM.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     58\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mreshape(x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], x\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28mprint\u001b[39m(x\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m---> 61\u001b[0m x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlstm1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     62\u001b[0m x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm2(x)\n\u001b[0;32m     63\u001b[0m x, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlstm3(x)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:810\u001b[0m, in \u001b[0;36mLSTM.forward\u001b[1;34m(self, input, hx)\u001b[0m\n\u001b[0;32m    806\u001b[0m     \u001b[38;5;66;03m# Each batch of the hidden state should match the input sequence that\u001b[39;00m\n\u001b[0;32m    807\u001b[0m     \u001b[38;5;66;03m# the user believes he/she is passing in.\u001b[39;00m\n\u001b[0;32m    808\u001b[0m     hx \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpermute_hidden(hx, sorted_indices)\n\u001b[1;32m--> 810\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_forward_args\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    811\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_sizes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    812\u001b[0m     result \u001b[38;5;241m=\u001b[39m _VF\u001b[38;5;241m.\u001b[39mlstm(\u001b[38;5;28minput\u001b[39m, hx, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_flat_weights, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_layers,\n\u001b[0;32m    813\u001b[0m                       \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdropout, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbidirectional, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:730\u001b[0m, in \u001b[0;36mLSTM.check_forward_args\u001b[1;34m(self, input, hidden, batch_sizes)\u001b[0m\n\u001b[0;32m    725\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcheck_forward_args\u001b[39m(\u001b[38;5;28mself\u001b[39m,  \u001b[38;5;66;03m# type: ignore[override]\u001b[39;00m\n\u001b[0;32m    726\u001b[0m                        \u001b[38;5;28minput\u001b[39m: Tensor,\n\u001b[0;32m    727\u001b[0m                        hidden: Tuple[Tensor, Tensor],\n\u001b[0;32m    728\u001b[0m                        batch_sizes: Optional[Tensor],\n\u001b[0;32m    729\u001b[0m                        ):\n\u001b[1;32m--> 730\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_sizes\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    731\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_hidden_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m    732\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[0] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m    733\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_hidden_size(hidden[\u001b[38;5;241m1\u001b[39m], \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mget_expected_cell_size(\u001b[38;5;28minput\u001b[39m, batch_sizes),\n\u001b[0;32m    734\u001b[0m                            \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExpected hidden[1] size \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\ProgramData\\anaconda3\\Lib\\site-packages\\torch\\nn\\modules\\rnn.py:218\u001b[0m, in \u001b[0;36mRNNBase.check_input\u001b[1;34m(self, input, batch_sizes)\u001b[0m\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    215\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput must have \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m dimensions, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    216\u001b[0m             expected_input_dim, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39mdim()))\n\u001b[0;32m    217\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m--> 218\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m    219\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minput.size(-1) must be equal to input_size. Expected \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m, got \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    220\u001b[0m             \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minput_size, \u001b[38;5;28minput\u001b[39m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)))\n",
      "\u001b[1;31mRuntimeError\u001b[0m: input.size(-1) must be equal to input_size. Expected 66, got 7680"
     ]
    }
   ],
   "source": [
    "obj2 = Hybrid_CNN_LSTM()\n",
    "print(obj2(torch.randn(16, 5, 62, 265)).shape)\n",
    "print(sum(n.numel() for n in obj2.parameters() if n.requires_grad))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3ab5ac",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7478f0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbb0a7b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4361f9d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccdf58ae",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c8752f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50df2e98",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c470dea",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71393c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c84e806",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e05cd7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c15db209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3d4915",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3061d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
